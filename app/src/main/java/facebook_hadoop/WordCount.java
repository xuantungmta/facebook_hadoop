/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package facebook_hadoop;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import facebook_hadoop.pipeline.Annotation;
import facebook_hadoop.pipeline.VnCoreNLP;

public class WordCount {
    private static VnCoreNLP pipeline;
    private static List<String> specCharacter = Arrays.asList("[", "]", ",", ".", "-", "+", ":", "^", "\"", "(", ")",
            "'", "&", "*", "$", "@", "%", "!", "#", "…", "?", "”", "“", "null", "-", ";", "/");
    private static List<String> stopWords;

    public static class DataExtractionMapper extends Mapper<Object, Text, Text, IntWritable> {

        private final static IntWritable one = new IntWritable(1);

        public void map(Object ikey, Text ivalue, Context context)
                throws IOException, InterruptedException {
            String row = ivalue.toString();
            String[] cells = row.split(",(?=([^\"]*\"[^\"]*\")*[^\"]*$)");
            try {
                String raw_text = cells[7];
                Annotation annotation = new Annotation(raw_text);
                pipeline.annotate(annotation);
                String[] list_sentences = annotation.toString().split("\n\n");
                for (String sent : list_sentences) {
                    String[] list_words = sent.split("\n");
                    for (String item : list_words) {
                        String word = item.replace("\t\t", "\t").split("\t")[1].replaceAll("\\.", "");
                        if (word == null || word.equals(""))
                            continue;
                        if (specCharacter.stream().anyMatch(word::equals))
                            continue;
                        if (stopWords.stream().anyMatch(word::equals))
                            continue;
                        context.write(new Text(word.trim().toLowerCase()), one);
                    }
                }

            } catch (Exception ex) {
            }
        }
    }

    public static class DataExtractionReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private final static IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        stopWords = new ArrayList<>();
        BufferedReader reader;
        try {
            // String pathStopwordFile = "./models/vietnamese-stopwords-dash.txt";
            String pathStopwordFile = "/home/mypc/scripts/facebook_hadoop/models/vietnamese-stopwords-dash.txt";
            reader = new BufferedReader(new FileReader(pathStopwordFile));
            String line = "";
            while (line != null) {
                line = reader.readLine();
                stopWords.add(line);
            }

            reader.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
        pipeline = new VnCoreNLP(new String[] { "wseg" });

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Facebook extraction");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(DataExtractionMapper.class);
        job.setReducerClass(DataExtractionReducer.class);
        // job.setSortComparatorClass(DescendingKeyComparator.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
